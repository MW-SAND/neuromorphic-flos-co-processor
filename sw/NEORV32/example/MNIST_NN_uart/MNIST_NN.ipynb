{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c53c3f2-f84f-41b5-b65c-cd6e703b9ce4",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Simple MNIST Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b96e03-5671-4105-8cf8-6e13fd01eece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 11:37:13.042280: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-16 11:37:13.046038: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-16 11:37:13.055637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734345433.073173    9647 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734345433.079098    9647 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-16 11:37:13.097803: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from fxpmath import Fxp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8f860-0d3a-421a-b848-52b573b27ff0",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Import MNIST dataset\n",
    "\n",
    "1. Import MNIST dataset\n",
    "2. Convert labels to one-hot encoding\n",
    "3. Flattern (reshape) and normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8ebbdd-19ce-417c-a5ed-962006763840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb56be87-6915-4b47-a93e-f17f7c60200f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape input to flat vector\n",
    "train_images = np.reshape(train_images, [-1, 28*28])\n",
    "test_images = np.reshape(test_images, [-1, 28*28])\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e3d4b-75f2-4dcb-b592-cf63f67b4a12",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Create model\n",
    "\n",
    "1. Define model parameters\n",
    "2. Define model architecture\n",
    "3. Compile the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f079ab-2648-4466-b2fb-fcc58f1ed784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "batch_size = 128\n",
    "hidden_units = 256\n",
    "dropout = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9f6cb9-53a4-40b9-a534-2bd7ff3ec291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiebren/school/final_project/.venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-12-16 11:37:16.748847: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,840</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m7,840\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,840</span> (30.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,840\u001b[0m (30.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,840</span> (30.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,840\u001b[0m (30.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_dim=28*28, use_bias=False),\n",
    "  tf.keras.layers.Dropout(dropout),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "908e015b-875a-49db-85c6-97d810dae63e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be757a8-2329-438f-8926-fd98db3db73e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e265a5d-6471-4139-b630-7b59d362ebe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 11:37:16.856421: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4712 - loss: 1.4857 - val_accuracy: 0.8985 - val_loss: 0.5090\n",
      "Epoch 2/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5740 - loss: 1.0862 - val_accuracy: 0.9084 - val_loss: 0.4179\n",
      "Epoch 3/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5839 - loss: 1.0512 - val_accuracy: 0.9143 - val_loss: 0.3838\n",
      "Epoch 4/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5877 - loss: 1.0290 - val_accuracy: 0.9156 - val_loss: 0.3632\n",
      "Epoch 5/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5923 - loss: 1.0232 - val_accuracy: 0.9165 - val_loss: 0.3571\n",
      "Epoch 6/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5936 - loss: 1.0112 - val_accuracy: 0.9170 - val_loss: 0.3457\n",
      "Epoch 7/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5938 - loss: 1.0134 - val_accuracy: 0.9175 - val_loss: 0.3372\n",
      "Epoch 8/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5946 - loss: 1.0051 - val_accuracy: 0.9186 - val_loss: 0.3343\n",
      "Epoch 9/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5924 - loss: 1.0140 - val_accuracy: 0.9195 - val_loss: 0.3328\n",
      "Epoch 10/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5942 - loss: 1.0210 - val_accuracy: 0.9179 - val_loss: 0.3322\n",
      "Epoch 11/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5943 - loss: 1.0026 - val_accuracy: 0.9198 - val_loss: 0.3281\n",
      "Epoch 12/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5981 - loss: 0.9984 - val_accuracy: 0.9187 - val_loss: 0.3285\n",
      "Epoch 13/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5975 - loss: 1.0033 - val_accuracy: 0.9188 - val_loss: 0.3275\n",
      "Epoch 14/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5919 - loss: 1.0103 - val_accuracy: 0.9201 - val_loss: 0.3261\n",
      "Epoch 15/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5958 - loss: 1.0067 - val_accuracy: 0.9182 - val_loss: 0.3259\n",
      "Epoch 16/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5993 - loss: 0.9889 - val_accuracy: 0.9206 - val_loss: 0.3231\n",
      "Epoch 17/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5967 - loss: 1.0040 - val_accuracy: 0.9207 - val_loss: 0.3214\n",
      "Epoch 18/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5992 - loss: 0.9962 - val_accuracy: 0.9193 - val_loss: 0.3241\n",
      "Epoch 19/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5979 - loss: 0.9972 - val_accuracy: 0.9208 - val_loss: 0.3189\n",
      "Epoch 20/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5964 - loss: 0.9972 - val_accuracy: 0.9213 - val_loss: 0.3169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fe3644592a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d98036c-9b93-4c2e-ba9e-c11f3830316a",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Manual inference calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c93531d-13e9-42aa-8612-995823531ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow inference:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "[ -4.8060937  -12.769396    -4.960964    -0.73131126  -5.6130304\n",
      "  -3.4489174   -9.644726     3.5483387   -3.149226    -2.1756184 ] = 7\n",
      "Manual inference:\n",
      "[ -4.806094   -12.769396    -4.960964    -0.73131126  -5.6130314\n",
      "  -3.4489176   -9.644726     3.548339    -3.1492262   -2.1756182 ] = 7\n",
      "Equality check:\n",
      "[ 4.7683716e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  9.5367432e-07  2.3841858e-07  0.0000000e+00 -2.3841858e-07\n",
      "  2.3841858e-07 -2.3841858e-07]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = test_images[0]\n",
    "\n",
    "plt.imshow(np.reshape(image, (28, 28)), \"gray\")\n",
    "\n",
    "print(\"Tensorflow inference:\")\n",
    "prediction = model.predict(np.array([image, ]))[0]\n",
    "print(f\"{prediction} = {prediction.argmax()}\")\n",
    "\n",
    "weights = np.array(model.weights[0])\n",
    "outputs = np.zeros(10, np.float32)\n",
    "\n",
    "for i in range(10):  # loop over neurons\n",
    "    for j in range(784):  # loop over input feature = image\n",
    "        outputs[i] += image[j] * weights[j, i]\n",
    "\n",
    "print(\"Manual inference:\")\n",
    "print(f\"{outputs} = {outputs.argmax()}\")\n",
    "\n",
    "print(\"Equality check:\")\n",
    "print(prediction - outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02fbde",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b778a-5380-491f-badc-5e9f2c31f177",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Write model weights to c header\n",
    "\n",
    "Writes the weights of the dense layers to a c header file like below\n",
    "\n",
    "```c\n",
    "double model_weights[][COLS] = {{0.5564, ..., 0.8794},\n",
    "                                :\n",
    "                                {0.7546, ..., 0.4868}};\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c99650c-5a01-41c9-bc46-fb0e6718994f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"neural_network.h\"\n",
    "\n",
    "headerfile = open(filename, \"w\")\n",
    "\n",
    "kernel_weights = np.array(model.weights[0])\n",
    "\n",
    "headerfile.write(f\"const float model_weights[][{kernel_weights.shape[1]}] = {{\")\n",
    "for i in range(kernel_weights.shape[0] - 1):\n",
    "    headerfile.write(\"{\")\n",
    "    for j in range(kernel_weights.shape[1] - 1):\n",
    "        headerfile.write(f\"{kernel_weights[i, j]}, \")\n",
    "    headerfile.write(f\"{kernel_weights[i, -1]}}},\\r\\n\")\n",
    "    headerfile.write((34 + len(str(kernel_weights.shape[1])))*' ')\n",
    "headerfile.write(\"{\")\n",
    "for j in range(kernel_weights.shape[1] - 1):\n",
    "    headerfile.write(f\"{kernel_weights[i, j]}, \")\n",
    "headerfile.write(f\"{kernel_weights[i, -1]}}}\")\n",
    "headerfile.write(\"};\\r\\n\")\n",
    "\n",
    "headerfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3128e6-f0b1-4afb-9483-8656ae4f860a",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Write image to c header\n",
    "\n",
    "Writes a selected image flaterned to a c header file like below\n",
    "\n",
    "```c\n",
    "double image[] = {0.5564, ..., 0.8794};\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "499525ea-1ad3-4bee-a86c-03c31d58f436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"image.h\"\n",
    "\n",
    "headerfile = open(filename, \"w\")\n",
    "\n",
    "headerfile.write(\"const float image[] = {\")\n",
    "for i in range(image.shape[0] - 1):\n",
    "    headerfile.write(f\"{image[i]}, \")\n",
    "headerfile.write(f\"{image[-1]}}};\\r\\n\")\n",
    "\n",
    "headerfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068af8b-4774-489c-b81b-ab793afb7ef9",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Write image and model weights to VHDL initialization file\n",
    "\n",
    "Writes selected image and model weights to VHDL file like below\n",
    "\n",
    "```vhdl\n",
    "-- Generated DMEM image with image and model weights\n",
    "-- Image start address = 0xXXXXXXXX\n",
    "-- Model weights start address = 0xXXXXXXXX\n",
    "\n",
    "package body neorv32_dmem_image is\n",
    "\n",
    "constant mem_ram_b0_init : mem8_t := (\n",
    "x\"AA\",\n",
    "  :\n",
    "x\"00\"\n",
    ");\n",
    "\n",
    "constant mem_ram_b1_init : mem8_t := (\n",
    "x\"BB\",\n",
    "  :\n",
    "x\"00\"\n",
    ");\n",
    "\n",
    "constant mem_ram_b2_init : mem8_t := (\n",
    "x\"CC\",\n",
    "  :\n",
    "x\"00\"\n",
    ");\n",
    "\n",
    "constant mem_ram_b3_init : mem8_t := (\n",
    "x\"DD\",\n",
    "  :\n",
    "x\"00\"\n",
    ");\n",
    "\n",
    "end neorv32_dmem_image;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9945b821-b981-446c-a06a-fa6a34069287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def float_as_int(f):\n",
    "    return struct.unpack('I', struct.pack('f', f))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2f5bfcb-6dcc-4962-a04b-178487da8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"neorv32_dmem_image_float.vhd\"\n",
    "starting_address = 0x80000400\n",
    "\n",
    "file = open(filename, \"w\")\n",
    "\n",
    "# write header\n",
    "file.write(\"-- Generated DMEM image with image and model weights\\n\")\n",
    "file.write(f\"-- Image at [0x{starting_address:0>8x}, 0x{(starting_address + 784*4 - 1):0>8x}]\\n\")\n",
    "file.write(f\"-- Model weights at [0x{(starting_address + 784*4):0>8x}, 0x{(starting_address + 784*4 + 10*784*4 - 1):0>8x}]\\n\")\n",
    "file.write(\"\\n\")\n",
    "file.write(\"package body neorv32_dmem_image is\\n\")\n",
    "\n",
    "for ram_i in range(4):\n",
    "    file.write(\"\\n\")\n",
    "    file.write(f\"constant mem_ram_b{ram_i}_init : mem8_t := (\\n\")\n",
    "\n",
    "    # write 0s up until starting address\n",
    "    for i in range(int((starting_address - 0x80000000) / 4)):\n",
    "        file.write(\"x\\\"00\\\",\\n\")\n",
    "\n",
    "    # write image\n",
    "    for i in range(image.shape[0]):\n",
    "        file.write(f\"x\\\"{((float_as_int(image[i]) >> (ram_i * 8)) & 0xFF):0>2x}\\\",\\n\")\n",
    "\n",
    "    # write model weights\n",
    "    for i in range(weights.shape[0] - 1):\n",
    "        for j in range(weights.shape[1]):\n",
    "            file.write(f\"x\\\"{((float_as_int(weights[i, j]) >> (ram_i * 8)) & 0xFF):0>2x}\\\",\\n\")\n",
    "    for j in range(weights.shape[1] - 1):\n",
    "        file.write(f\"x\\\"{((float_as_int(weights[-1, j]) >> (ram_i * 8)) & 0xFF):0>2x}\\\",\\n\")\n",
    "    file.write(f\"x\\\"{((float_as_int(weights[-1, -1]) >> (ram_i * 8)) & 0xFF):0>2x}\\\"\\n\")\n",
    "\n",
    "    file.write(\");\\n\")\n",
    "\n",
    "# write footer\n",
    "file.write(\"\\n\")\n",
    "file.write(\"end neorv32_dmem_image;\\n\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a20831-ea3f-455f-af76-18b7fdb87194",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Write output to txt file for verification\n",
    "\n",
    "```\n",
    "[0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "974b136d-fee1-4238-a11d-49ca5ee73d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"verification_float.out\"\n",
    "\n",
    "file = open(filename, \"w\")\n",
    "\n",
    "file.write(\"[\")\n",
    "for i in range(9):\n",
    "    file.write(f\"0x{float_as_int(outputs[i]):0>8x}, \")\n",
    "file.write(f\"0x{float_as_int(outputs[9]):0>8x}]\\n\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbee76d",
   "metadata": {},
   "source": [
    "# Fixed point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81f6e7",
   "metadata": {},
   "source": [
    "## Convert model to fixed point and calculate accuracy\n",
    "\n",
    "used fxpmath python module for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4636be9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fxp acc = 0.9216\n"
     ]
    }
   ],
   "source": [
    "fxp_ref = Fxp(None, dtype='fxp-s16/7') # 16 bit = 1 sign bit, 8 integer, and 7 fractional\n",
    "\n",
    "fxp_weights = Fxp(weights, like=fxp_ref)\n",
    "fxp_outputs = Fxp(np.zeros(10), like=fxp_ref)\n",
    "\n",
    "correct = 0\n",
    "for image, label in zip(test_images, test_labels):\n",
    "    fxp_image = Fxp(image, like=fxp_ref)\n",
    "\n",
    "    raw_outputs = fxp_weights.val.T.dot(fxp_image.val) >> 7\n",
    "    fxp_outputs.set_val(raw_outputs, raw=True)\n",
    "\n",
    "    fxp_label = fxp_outputs.argmax()\n",
    "    correct += fxp_label == label\n",
    "\n",
    "fxp_accuracy = correct / len(test_images)\n",
    "print(f\"fxp acc = {fxp_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efc53206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fxp inference:\n",
      "[ -4.7578125 -12.5234375  -4.8828125  -0.7578125  -5.4609375  -3.390625\n",
      "  -9.3984375   3.4453125  -3.09375    -2.1171875] = 7\n"
     ]
    }
   ],
   "source": [
    "fxp_image = Fxp(test_images[0], like=fxp_ref)\n",
    "\n",
    "raw_outputs = fxp_weights.val.T.dot(fxp_image.val) >> 7\n",
    "fxp_outputs.set_val(raw_outputs, raw=True)\n",
    "\n",
    "print(f\"fxp inference:\")\n",
    "print(f\"{fxp_outputs} = {fxp_outputs.argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eddb5e",
   "metadata": {},
   "source": [
    "## Write fixed point image and model weights to VHDL initialization file\n",
    "\n",
    "Writes selected image and model weights in fixed point to VHDL file like below\n",
    "\n",
    "```vhdl\n",
    "-- Generated DMEM image with image and model weights\n",
    "-- Image start address = 0xXXXXXXXX\n",
    "-- Model weights start address = 0xXXXXXXXX\n",
    "\n",
    "package body neorv32_dmem_image is\n",
    "\n",
    "constant mem_ram_b0_init : mem8_t := (\n",
    "x\"AA\",\n",
    "  :\n",
    "x\"00\"\n",
    ");\n",
    "\n",
    "constant mem_ram_b1_init : mem8_t := (\n",
    "x\"BB\",\n",
    "  :\n",
    "x\"00\"\n",
    ");\n",
    "\n",
    "constant mem_ram_b2_init : mem8_t := (\n",
    "x\"CC\",\n",
    "  :\n",
    "x\"00\"\n",
    ");\n",
    "\n",
    "constant mem_ram_b3_init : mem8_t := (\n",
    "x\"DD\",\n",
    "  :\n",
    "x\"00\"\n",
    ");\n",
    "\n",
    "end neorv32_dmem_image;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5913596",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"neorv32_dmem_image_fxp.vhd\"\n",
    "starting_address = 0x80000400\n",
    "\n",
    "file = open(filename, \"w\")\n",
    "\n",
    "# write header\n",
    "file.write(\"-- Generated DMEM image with fixed point image and model weights\\n\")\n",
    "file.write(f\"-- Image at [0x{starting_address:0>8x}, 0x{(starting_address + 784*4 - 1):0>8x}]\\n\")\n",
    "file.write(f\"-- Model weights at [0x{(starting_address + 784*4):0>8x}, 0x{(starting_address + 784*4 + 10*784*4 - 1):0>8x}]\\n\")\n",
    "file.write(\"\\n\")\n",
    "file.write(\"package body neorv32_dmem_image is\\n\")\n",
    "\n",
    "for ram_i in range(4):\n",
    "    file.write(\"\\n\")\n",
    "    file.write(f\"constant mem_ram_b{ram_i}_init : mem8_t := (\\n\")\n",
    "\n",
    "    # write 0s up until starting address\n",
    "    for i in range(int((starting_address - 0x80000000) / 4)):\n",
    "        file.write(\"x\\\"00\\\",\\n\")\n",
    "\n",
    "    # write fxp image\n",
    "    for i in range(fxp_image.shape[0]):\n",
    "        file.write(f\"x\\\"{((fxp_image[i].val >> (ram_i * 8)) & 0xFF):0>2x}\\\",\\n\")\n",
    "\n",
    "    # write fxp model weights\n",
    "    for i in range(fxp_weights.shape[0] - 1):\n",
    "        for j in range(fxp_weights.shape[1]):\n",
    "            file.write(f\"x\\\"{((fxp_weights[i, j].val >> (ram_i * 8)) & 0xFF):0>2x}\\\",\\n\")\n",
    "    for j in range(fxp_weights.shape[1] - 1):\n",
    "        file.write(f\"x\\\"{((fxp_weights[-1, j].val >> (ram_i * 8)) & 0xFF):0>2x}\\\",\\n\")\n",
    "    file.write(f\"x\\\"{((fxp_weights[-1, -1].val >> (ram_i * 8)) & 0xFF):0>2x}\\\"\\n\")\n",
    "\n",
    "    file.write(\");\\n\")\n",
    "\n",
    "# write footer\n",
    "file.write(\"\\n\")\n",
    "file.write(\"end neorv32_dmem_image;\\n\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a804b32",
   "metadata": {},
   "source": [
    "## Write output to txt file for verification\n",
    "\n",
    "```\n",
    "[0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX, 0xXXXXXXXX]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96ba3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_as_uint(i):\n",
    "    return struct.unpack('I', struct.pack('i', i))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e4d6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"verification_fxp.out\"\n",
    "\n",
    "file = open(filename, \"w\")\n",
    "\n",
    "file.write(\"[\")\n",
    "for i in range(9):\n",
    "    file.write(f\"0x{int_as_uint(fxp_outputs[i].val):0>8x}, \")\n",
    "file.write(f\"0x{int_as_uint(fxp_outputs[9].val):0>8x}]\\n\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51c73f",
   "metadata": {},
   "source": [
    "# Quantize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba293fc-65d4-423c-9062-872629d9cc42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to int8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf22042-7e70-4aed-b84d-742df4600d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.lite.experimental.Analyzer.analyze(model_content=tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9957a-a5df-428e-834b-85b24255fc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model, experimental_preserve_all_tensors=True)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62de29e-3b11-40e1-ba8e-b9cad22281a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image = image\n",
    "\n",
    "plt.imshow(np.reshape(test_image, (28, 28)), \"gray\")\n",
    "\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "output_scale, output_zero_point = output_details[\"quantization\"]\n",
    "output_dequant = output_scale * (output - output_zero_point).astype(np.float32)\n",
    "\n",
    "print(\"Tensorflow lite inference:\")\n",
    "print(output, f\"= {output.argmax()}\")\n",
    "print(output_dequant, f\"= {output_dequant.argmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be18492-863e-4fe1-b640-174c6224bd35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(interpreter.get_tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a90920-de34-4192-9ef3-39e11f83ab13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantized_matmul_and_add(input_data, weights, biases, input_scale,\n",
    "                             input_zero_point, weight_scale, weight_zero_point,\n",
    "                             bias_scale, output_scale, output_zero_point):\n",
    "    # Perform integer matrix multiplication\n",
    "    # Note: We assume weights and inputs are in int8 format\n",
    "    # Convert inputs and weights to int32 for safe multiplication\n",
    "    input_data = input_data.astype(np.int32)\n",
    "    weights = weights.astype(np.int32)\n",
    "\n",
    "    # Calculate the scale factor for the result of the matrix multiplication\n",
    "    scale_factor = (input_scale * weight_scale) / output_scale\n",
    "\n",
    "    # Perform the matrix multiplication\n",
    "    result = np.dot(input_data - input_zero_point, (weights - weight_zero_point).T)\n",
    "\n",
    "    # Adjust result by scale factor\n",
    "    result = result * scale_factor\n",
    "\n",
    "    # Add biases (converted to int32 for safe addition)\n",
    "    result += biases * (bias_scale / output_scale)\n",
    "\n",
    "    # Add output_zero_point\n",
    "    result += output_zero_point\n",
    "\n",
    "    # Clip to int8 range\n",
    "    result = np.clip(result, -128, 127).astype(np.int8)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage with dummy data\n",
    "# Assuming 'image' is your input image in int8 format\n",
    "quantized_image = test_image[0]\n",
    "\n",
    "# Dummy weights and biases for demonstration purposes\n",
    "weights_1 = interpreter.get_tensor(4)\n",
    "biases_1 = interpreter.get_tensor(3)\n",
    "\n",
    "weights_2 = interpreter.get_tensor(2)\n",
    "biases_2 = interpreter.get_tensor(1)\n",
    "\n",
    "# Quantization parameters from your model description\n",
    "(input_scale_1, input_zero_point_1) = interpreter.get_tensor_details()[0][\"quantization\"]\n",
    "# input_scale_1 = 0.003921568859368563\n",
    "# input_zero_point_1 = -128\n",
    "\n",
    "(weight_scale_1, weight_zero_point_1) = interpreter.get_tensor_details()[4][\"quantization\"]\n",
    "# weight_scale_1 = 0.007190252188593149\n",
    "# weight_zero_point_1 = 0\n",
    "\n",
    "(_, bias_scale_1) = interpreter.get_tensor_details()[3][\"quantization\"]\n",
    "# bias_scale_1 = 2.8197069696034305e-05\n",
    "\n",
    "(output_scale_1, output_zero_point_1) = interpreter.get_tensor_details()[5][\"quantization\"]\n",
    "# output_scale_1 = 0.028666598722338676\n",
    "# output_zero_point_1 = -128\n",
    "\n",
    "# Layer 1 inference in quantized domain\n",
    "layer_1_output = quantized_matmul_and_add(\n",
    "    quantized_image,\n",
    "    weights_1,\n",
    "    biases_1,\n",
    "    input_scale_1,\n",
    "    input_zero_point_1,\n",
    "    weight_scale_1,\n",
    "    weight_zero_point_1,\n",
    "    bias_scale_1,\n",
    "    output_scale_1,\n",
    "    output_zero_point_1\n",
    ")\n",
    "\n",
    "# Apply ReLU in quantized domain (clip negative values to zero)\n",
    "layer_1_output[layer_1_output < output_zero_point_1] = output_zero_point_1\n",
    "\n",
    "print(layer_1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b5441-6a0f-4610-b601-915a656d5139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantization parameters for Layer 2\n",
    "input_scale_2 = output_scale_1\n",
    "input_zero_point_2 = output_zero_point_1\n",
    "\n",
    "(weight_scale_2, weight_zero_point_2) = interpreter.get_tensor_details()[2][\"quantization\"]\n",
    "# weight_scale_2 = 0.008775909431278706\n",
    "# weight_zero_point_2 = 0\n",
    "\n",
    "(_, bias_scale_2) = interpreter.get_tensor_details()[1][\"quantization\"]\n",
    "# bias_scale_2 = 0.00025157546042464674\n",
    "\n",
    "(output_scale_2, output_zero_point_2) = interpreter.get_tensor_details()[6][\"quantization\"]\n",
    "# output_scale_2 = 0.26393115520477295\n",
    "# output_zero_point_2 = 38\n",
    "\n",
    "# Layer 2 inference in quantized domain\n",
    "final_output_quantized = quantized_matmul_and_add(\n",
    "    layer_1_output,\n",
    "    weights_2,\n",
    "    biases_2,\n",
    "    input_scale_2,\n",
    "    input_zero_point_2,\n",
    "    weight_scale_2,\n",
    "    weight_zero_point_2,\n",
    "    bias_scale_2,\n",
    "    output_scale_2,\n",
    "    output_zero_point_2\n",
    ")\n",
    "\n",
    "print(final_output_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd666f9-ae54-4187-ad9e-1383760ee8d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpreter.get_signature_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866e07c-e00c-4e0f-9b8f-93d7ebba2b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = interpreter.get_signature_runner()\n",
    "f(dense_input=test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a9707-a8e7-435d-b8ca-1d05f3a3e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((input_scale_1 * weight_scale_1) / output_scale_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f31854-3aaf-4f42-9bf3-7502454c430e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpreter.get_tensor_details()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
